{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f988c9b0-1909-4bbb-ace7-de7bee0fe244",
   "metadata": {},
   "source": [
    "# Semantic segmentation using PyTorch and TorchVision\n",
    "\n",
    "## Overview\n",
    "In this lab session, we will explore semantic segmentation and learn how to obtain the segmented masks using the utilities provided by PyTorch and TorchVision. \n",
    "\n",
    "## Learning objectives\n",
    "- Understand how to visualise images using TorchVision utilities.\n",
    "- Explore semantic segmentation using pre-trained models.\n",
    "- Use segmentation masks for visualising model predictions.\n",
    "\n",
    "## Steps\n",
    "1. Import the required libraries and define helper functions.\n",
    "2. Visualize a grid of images.\n",
    "3. Use semantic segmentation models and visualise their outputs.\n",
    "4. Apply segmentation masks to images.\n",
    "5. Analyze the model predictions for multiple classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aafad51-b156-485a-8223-e6e747611fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### **Cell 1: Import Libraries**\n",
    "# Import essential libraries for tensor manipulation, visualization, and image transformations.\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "plt.rcParams[\"savefig.bbox\"] = 'tight'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be14007-1483-47d7-bd5d-37d11666acab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### **Cell 2: Define `show` Utility**\n",
    "# A utility function to display one or more images using Matplotlib.\n",
    "def show(imgs):\n",
    "    if not isinstance(imgs, list):\n",
    "        imgs = [imgs]\n",
    "    fig, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n",
    "    for i, img in enumerate(imgs):\n",
    "        img = img.detach()\n",
    "        img = F.to_pil_image(img)\n",
    "        axs[0, i].imshow(np.asarray(img))\n",
    "        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5e3f3b-01f1-49e5-8680-29555b9b51d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### **Cell 3: Visualizing a Grid of Images**\n",
    "# Load and decode images for visualization.\n",
    "\n",
    "from torchvision.io import read_file, decode_image\n",
    "from pathlib import Path\n",
    "\n",
    "image_folder = Path('./images')\n",
    "dog1_int = decode_image(read_file(str(image_folder / 'dog1.jpg')))\n",
    "dog2_int = decode_image(read_file(str(image_folder / 'dog2.jpg')))\n",
    "dog_list = [dog1_int, dog2_int]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d30c96-314c-46c1-8e88-2fdc14d1c5b4",
   "metadata": {},
   "source": [
    "### Semantic segmentation models\n",
    "\n",
    "We will see how to use it with torchvision's FCN Resnet-50, loaded with `torchvision.models.segmentation.fcn_resnet50`. Let's start by looking at the output of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07416d2-6148-47e5-8b90-a3d67f61a9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### **Cell 4: Semantic Segmentation with FCN ResNet-50**\n",
    "# Use a pre-trained FCN ResNet-50 model to perform semantic segmentation and output prediction scores.\n",
    "\n",
    "from torchvision.models.segmentation import fcn_resnet50, FCN_ResNet50_Weights\n",
    "\n",
    "weights = FCN_ResNet50_Weights.DEFAULT\n",
    "transforms = weights.transforms(resize_size=None)\n",
    "\n",
    "model = fcn_resnet50(weights=weights, progress=False)\n",
    "model = model.eval()\n",
    "\n",
    "batch = torch.stack([transforms(d) for d in dog_list])\n",
    "output = model(batch)['out']\n",
    "print(output.shape, output.min().item(), output.max().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82189ea8-6020-429c-b291-74eea9ca570d",
   "metadata": {},
   "source": [
    "As we can see above, the output of the segmentation model is a tensor of shape ``(batch_size, num_classes, H, W)``. Each value is a non-normalized score, and we can normalize them into ``[0, 1]`` by using a softmax. After the softmax, we can interpret each value as a probability indicating how likely a given pixel is to belong to a given class.\n",
    "\n",
    "Let's plot the masks that have been detected for the dog class and for the boat class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdd0c78-5a0c-43ba-96f0-069291186979",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### **Cell 5: Normalize and Visualize Masks**\n",
    "# Normalize segmentation outputs to probabilities and visualize masks for specific classes.\n",
    "\n",
    "sem_class_to_idx = {cls: idx for (idx, cls) in enumerate(weights.meta[\"categories\"])}\n",
    "\n",
    "normalized_masks = torch.nn.functional.softmax(output, dim=1)\n",
    "print(normalized_masks.shape, normalized_masks.min().item(), normalized_masks.max().item())\n",
    "\n",
    "dog_and_boat_masks = [\n",
    "    normalized_masks[img_idx, sem_class_to_idx[cls]]\n",
    "    for img_idx in range(len(dog_list))\n",
    "    for cls in ('dog', 'boat')\n",
    "]\n",
    "\n",
    "show(dog_and_boat_masks)\n",
    "# from left to right \n",
    "# image1 dog_class, image1 boat_class, image1 dog_class, image1 boat_class,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef19477-765a-414b-8c49-95a0579ea432",
   "metadata": {},
   "source": [
    "### Visualizing binary segmentation masks for the dog class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc9bf2a-e9ab-4945-8733-5f79e1522588",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### **Cell 6: Binary Masks and Segmentation Visualization**\n",
    "# Create binary masks for the \"dog\" class and visualize the segmented regions.\n",
    "class_dim = 1\n",
    "boolean_dog_masks = (normalized_masks.argmax(class_dim) == sem_class_to_idx['dog'])\n",
    "print(f\"shape = {boolean_dog_masks.shape}, dtype = {boolean_dog_masks.dtype}\")\n",
    "show([m.float() for m in boolean_dog_masks])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a108e77-f4e3-4931-adc1-f366747dab0c",
   "metadata": {},
   "source": [
    "### Visualizing segmentation masks\n",
    "The `~torchvision.utils.draw_segmentation_masks` function can be used to draw segmentation masks on images. Semantic segmentation and instance segmentation models have different outputs, so we will treat each independently.\n",
    "\n",
    "Now that we have boolean masks, we can use them with `~torchvision.utils.draw_segmentation_masks` to plot them on top of the\n",
    "original images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ba411d-e28e-46dd-811b-e146b5430619",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### **Cell 7: Overlay Segmentation Masks**\n",
    "# Overlay segmentation masks on original images to combine predictions with visual context.\n",
    "\n",
    "from torchvision.utils import draw_segmentation_masks\n",
    "\n",
    "dogs_with_masks = [\n",
    "    draw_segmentation_masks(img, masks=mask, alpha=0.7)\n",
    "    for img, mask in zip(dog_list, boolean_dog_masks)\n",
    "]\n",
    "show(dogs_with_masks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f08b41-3557-47ae-8012-8b2e04c80aef",
   "metadata": {},
   "source": [
    "### More than one mask\n",
    "We can plot more than one mask per image! Remember that the model returned as many masks as there are classes. Let's ask the same query as above, but this time for *all* classes, not just the dog class: \"For each pixel and each class C, is class C the most likely class?\"\n",
    "\n",
    "This one is a bit more involved, so we'll first show how to do it with a single image, and then we'll generalize to the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6631c889-7d1d-48ef-9cc1-82abdd1d199c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### **Cell 8: Multi-class Segmentation**\n",
    "#  Visualize masks for all detected classes \n",
    "num_classes = normalized_masks.shape[1]\n",
    "dog1_masks = normalized_masks[0]\n",
    "class_dim = 0\n",
    "dog1_all_classes_masks = dog1_masks.argmax(class_dim) == torch.arange(num_classes)[:, None, None]\n",
    "\n",
    "print(f\"dog1_masks shape = {dog1_masks.shape}, dtype = {dog1_masks.dtype}\")\n",
    "print(f\"dog1_all_classes_masks = {dog1_all_classes_masks.shape}, dtype = {dog1_all_classes_masks.dtype}\")\n",
    "\n",
    "dog_with_all_masks = draw_segmentation_masks(dog1_int, masks=dog1_all_classes_masks, alpha=.7)\n",
    "show(dog_with_all_masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358f7763-9406-4ad8-b24d-d9e9f8fca432",
   "metadata": {},
   "source": [
    "### Masks for the batch\n",
    "We can see in the image above that only 2 masks were drawn: the mask for the background and the mask for the dog. This is because the model thinks that only these 2 classes are the most likely ones across all the pixels. If the model had detected another class as the most likely among other pixels, we would have seen its mask above.\n",
    "\n",
    "Removing the background mask is as simple as passing ``masks=dog1_all_classes_masks[1:]``, because the background class is the class with index 0.\n",
    "\n",
    "Let's now do the same but for an entire batch of images. The code is similar but involves a bit more juggling with the dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46eeed8c-494e-423e-b614-faf62d4596fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### **Cell 10: Multi-class Segmentation**\n",
    "#  Visualize masks for all detected classes across the batch of images.\n",
    "class_dim = 1\n",
    "all_classes_masks = normalized_masks.argmax(class_dim) == torch.arange(num_classes)[:, None, None, None]\n",
    "print(f\"shape = {all_classes_masks.shape}, dtype = {all_classes_masks.dtype}\")\n",
    "# The first dimension is the classes now, so we need to swap it\n",
    "all_classes_masks = all_classes_masks.swapaxes(0, 1)\n",
    "\n",
    "dogs_with_masks = [\n",
    "    draw_segmentation_masks(img, masks=mask, alpha=.7)\n",
    "    for img, mask in zip(dog_list, all_classes_masks)\n",
    "]\n",
    "show(dogs_with_masks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c751d39f-3f42-42a6-934f-08daf514b32d",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "Modify the code to load and visualise the images `Mexico-City.jpg` and `Street.jpg` included in the images folder, and complete the following tasks:\n",
    "\n",
    "### **Task 1. Visualise more classes (probability maps)**\n",
    "Extend the code in cell 5 to correctly visualise three different classes from each image. Explore the different classes in `sem_class_to_idx` and select at least one that is not present in the images.\n",
    "\n",
    "### **Task 2. Visualise binary segmentation masks**\n",
    "Extend the code in cell 6 to correctly visualise the binary map belonging to the class with the highest probability on each image.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363e1c69-358b-47aa-a9ac-e925ae0fd66e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
