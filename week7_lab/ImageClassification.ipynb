{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Image Classification using PyTorch\n",
    "\n",
    "In this lab session, you will build a basic Convolutional Neural Network (CNN) using PyTorch. The CNN will classify images stored in the `Data/training_set` and `Data/test_set` folders. We will follow the steps of data preprocessing, defining a simple CNN, training the model, and evaluating its performance.\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "PyTorch is one of the most widely used deep learning libraries that you can use to build a CNN. CNNs are particularly effective for image recognition tasks, making them ideal for applications such as facial recognition, self-driving cars, and medical image analysis. In this lab, we will work through the process of building a basic CNN for binary image classification, similar to the popular \"Cats vs. Dogs\" classification problem.\n",
    "\n",
    "By the end of this session, you will understand how to:\n",
    "\n",
    "- Define and initialize a CNN model in PyTorch.\n",
    "- Apply convolution and pooling operations to extract features from images.\n",
    "- Use data augmentation to increase the robustness of the model.\n",
    "- Train the CNN model using the binary cross-entropy loss function and the Adam optimizer.\n",
    "- Evaluate the model's performance on a test dataset.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By completing this lab, students will be able to:\n",
    "\n",
    "1. Understand the architecture and components of Convolutional Neural Networks (CNNs).\n",
    "2. Implement a CNN in PyTorch to classify images.\n",
    "3. Perform data augmentation techniques to enhance the model's performance and prevent overfitting.\n",
    "4. Train and evaluate the CNN model using PyTorch's training loop and metrics.\n",
    "5. Visualize and interpret model accuracy and performance metrics.\n",
    "\n",
    "This exercise builds a strong foundation in deep learning and prepares students for more advanced topics in neural networks, such as transfer learning and object detection.\n",
    "\n",
    "## Instructions\n",
    "\n",
    "Please follow these steps to complete the lab:\n",
    "\n",
    "1. **Step 1: Data processing**\n",
    "\n",
    "2. **Step 2: Model definition**\n",
    "\n",
    "3. **Step 3: Loss and Optimizer**\n",
    "\n",
    "4. **Step 4: Training the model**\n",
    "\n",
    "5. **Step 5: Evaluating the model**\n",
    "\n",
    "6. **Step 6: Testing a classification example**\n",
    "\n",
    "7. **Step 7: Create a report**\n",
    "\n",
    "You are asked to write a short (no more than 2 pages) report of your work, completing the tasks and answering the questions. This work is not assessed (it will not count towards your module mark) but you will get formative feedback. Make sure to refer to the comments in the code for guidance. Feel free to experiment with different architectures or hyperparameters (e.g., number of layers, number of filters, learning rate) to see how they affect the performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Data Preprocessing**:\n",
    "   - We define a `transform` to resize images to `64x64` and normalize their pixel values. This step is important for handling images consistently.\n",
    "   - `datasets.ImageFolder` is used to load images from the `data_set_train` and `data_set_test` directories. Images are organized into subfolders named after the classes they belong to (e.g., `cat` and `dog`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before following the steps 1-6, let's take a look at one of the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read image\n",
    "cat10 = skimage.io.imread('Data/training_set/cats/cat.10.jpg')\n",
    "\n",
    "# Display the image\n",
    "plt.imshow(cat10)\n",
    "plt.title(\"Original Image\")\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Data Preprocessing\n",
    "# Define transformations for the training and testing datasets\n",
    "# Transforms include resizing the images to 64x64, converting them to tensors, and normalizing pixel values\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),  # Resize images to 64x64 pixels\n",
    "    transforms.ToTensor(),  # Convert image to PyTorch tensor\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize images (important for deep learning models)\n",
    "])\n",
    "\n",
    "# Load the training and testing datasets from the directories\n",
    "train_set = datasets.ImageFolder(root='Data/training_set', transform=transform)\n",
    "test_set = datasets.ImageFolder(root='Data/test_set', transform=transform)\n",
    "print(\"Transformations generated\")\n",
    "\n",
    "# Create data loaders to load data in batches\n",
    "train_loader = DataLoader(train_set, batch_size=32, shuffle=True)  # Shuffle to randomize data\n",
    "test_loader = DataLoader(test_set, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Model Definition**:\n",
    "   - We use `nn.Sequential` to create a basic Convolutional Neural Network (CNN) with two convolutional layers and two fully connected layers.\n",
    "   - The `Conv2d` layers detect patterns, while the `ReLU` activation introduces non-linearity. `MaxPool2d` reduces the size of feature maps.\n",
    "   - `Flatten` transforms the 2D feature maps into a 1D vector to be fed into the fully connected layers.\n",
    "   - The final layer is a `Softmax` layer for multi-class classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define the Model\n",
    "# Create a simple Convolutional Neural Network (CNN) for image classification\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1),  # First conv layer\n",
    "    nn.ReLU(),  # Activation function\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2),  # Pooling layer (reduce size)\n",
    "\n",
    "    nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1),  # Second conv layer\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2),  # Pooling\n",
    "\n",
    "    nn.Flatten(),  # Flatten the output from the conv layers for the fully connected layer\n",
    "    nn.Linear(32 * 16 * 16, 64),  # Fully connected layer\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 2),  # Output layer (assuming 2 classes in the dataset)\n",
    "    nn.Softmax(dim=1)  # Softmax for classification\n",
    ")\n",
    "\n",
    "print(\"Model created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Loss and Optimizer**:\n",
    "   - `CrossEntropyLoss` is used because it's well-suited for classification tasks where the output is a probability distribution across multiple classes.\n",
    "   - `Adam` optimizer adjusts the weights using gradients, with a learning rate of `0.001`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Define Loss Function and Optimizer\n",
    "# Use Cross Entropy Loss for classification tasks and Adam optimizer\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()  # Loss function for multi-class classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Training the Model**:\n",
    "   - The training loop runs for a specified number of epochs, where each image in the dataset is passed through the model to compute loss, and the optimizer updates the weights.\n",
    "   - We print the loss at the end of each epoch to monitor training progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Train the Model\n",
    "num_epochs = 5  # Number of times to go through the dataset\n",
    "print(\"Starting training... \")\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass: compute the output\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass: compute the gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Add loss to the running total\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Print the average loss for this epoch\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}')\n",
    "\n",
    "print('...Training completed!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. **Evaluating the Model**:\n",
    "   - During evaluation, the model predicts the class of each image in the test set, and the accuracy is calculated by comparing predictions to the ground truth labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Evaluate the Model\n",
    "# Set the model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():  # No need to compute gradients during evaluation\n",
    "    for inputs, labels in test_loader:\n",
    "        # Forward pass: compute the output\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Get the predicted class (the class with the highest score)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        # Update the total number of images and correct predictions\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "# Print the final accuracy\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Accuracy on the test set: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. **Evaluating the Model**:\n",
    "   - Using the trained classifier on a single image from the test set to predict its label. This includes passing the image through the model and obtaining the predicted label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Use the Classifier on a Single Image\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Function to show an image\n",
    "def my_imshow(img):\n",
    "    img = img / 2 + 0.5  # Unnormalize the image\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "# Get a batch of test data\n",
    "dataiter = iter(test_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# Show the first image in the batch\n",
    "my_imshow(images[0])\n",
    "\n",
    "# Pass the first image through the model to get the predicted label\n",
    "model.eval()  # Make sure model is in evaluation mode\n",
    "with torch.no_grad():  # Disable gradient calculation\n",
    "    outputs = model(images[0].unsqueeze(0))  # Add a batch dimension (unsqueeze)\n",
    "    _, predicted = torch.max(outputs, 1)  # Get the index of the highest score (prediction)\n",
    "\n",
    "# Get the class names from the dataset\n",
    "class_names = test_set.classes  # Class names are taken from the folder names\n",
    "\n",
    "# Print the predicted label and the actual label\n",
    "print(f'Predicted label: {class_names[predicted.item()]}')\n",
    "print(f'Actual label: {class_names[labels[0].item()]}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task1\n",
    "### Modify image sizes\n",
    "\n",
    "Currently, the images are resized to 64x64 pixels before being fed into the model. Modify the code to resize the images to 128x128 pixels instead, and observe how this affects the model's performance.\n",
    "Question: What effect does increasing the image size have on the training time and accuracy? Why might this happen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete Task 1 here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "### Change the Number of Convolutional Filters\n",
    "\n",
    "In the current model, the first convolutional layer has 16 filters, and the second convolutional layer has 32 filters. Change the number of filters to 32 and 64, respectively.\n",
    "Question: How does changing the number of filters affect the model's accuracy? What might be the reason for this change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete Task 2 here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3\n",
    "### Add a Dropout Layer\n",
    "Overfitting can be an issue in deep learning models. Add a `Dropout` layer between the fully connected layers with a dropout probability of 0.5 to reduce overfitting. Re-train the model and evaluate its performance.\n",
    "Question: How does adding a dropout layer affect the model's training and test accuracy? Explain why dropout can help prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete Task 3 here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4. \n",
    "### Add a third convolutional layer\n",
    "Add a third convolutional layer to the model with 128 filters, observe how this affects the model’s performance and training time, and describe what you found. Use a similar structure as the previous layers with a `Conv2d` layer, `ReLU`, and `MaxPool2d`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete Task 4 here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "74a34817a5cd2140ee5dc8be93178d611a51b0c87e7dbd285328f223a7951b22"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
